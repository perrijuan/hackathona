# ğŸ“Œ Vector â€“ Plataforma de Caronas UniversitÃ¡rias  

Vector Ã© um aplicativo open source feito por universitÃ¡rios e para universitÃ¡rios, com foco em acessibilidade, seguranÃ§a e inclusÃ£o no transporte estudantil.  
O projeto nasceu em um hackathon com a missÃ£o de reduzir desigualdades no acesso ao ensino superior, oferecendo caronas seguras, econÃ´micas e colaborativas.  

## Imagens 
<img width="1885" height="916" alt="Screenshot_20250827_215702" src="https://github.com/user-attachments/assets/a169cfbd-caa4-46a1-9b8b-c7c9566fc73f" />
<img width="1910" height="915" alt="Screenshot_20250827_220150" src="https://github.com/user-attachments/assets/2d33ba35-11e4-4e3c-b7d8-296bccf92ed5" />
<img width="1920" height="932" alt="Screenshot_20250827_221529" src="https://github.com/user-attachments/assets/b23a5a48-4ad7-4d60-84b5-06798fa5f397" />
<img width="1910" height="915" alt="Screenshot_20250827_220150" src="https://github.com/user-attachments/assets/95ced67c-088f-4874-8920-84c39be32183" />
<img width="1897" height="923" alt="Screenshot_20250827_220031" src="https://github.com/user-attachments/assets/1ab7314e-585a-4b06-820e-d3a47eb4c221" />


## âœ¨ Funcionalidades  

- ğŸš— Carona SolidÃ¡ria â€“ Sistema de caronas gratuitas ou a baixo custo para estudantes em vulnerabilidade.  
- â¤ï¸ Comunidade Segura â€“ AvaliaÃ§Ã£o de motoristas, chat direto e hubs de encontro seguro.  
- ğŸ¤ Open Source â€“ CÃ³digo aberto para que qualquer pessoa possa auditar, contribuir e expandir.  
- ğŸ’¸ Baixo Custo Real â€“ PreÃ§os acessÃ­veis, pensados para a realidade financeira de estudantes.  
- ğŸ† GamificaÃ§Ã£o (futuro) â€“ Pontos e recompensas para motoristas e usuÃ¡rios ativos.  

## ğŸ”— Links Ãšteis  

- RepositÃ³rio GitHub: https://github.com/claudio-asj/hackathona  
- Deploy (Vercel): https://hackatona.vercel.app/  
- Deploy de Teste (Vercel Dev): https://hackaton-dev.vercel.app/

## ğŸ› ï¸ Tecnologias  

- âš¡ Vite â€“ Bundler rÃ¡pido e moderno  
- âš›ï¸ React com TypeScript  
- ğŸ¨ Shadcn/UI â€“ Componentes acessÃ­veis e estilizados  
- ğŸ”¥ Firebase â€“ AutenticaÃ§Ã£o, banco de dados e hosting  
- ğŸ­ Tailwind CSS â€“ EstilizaÃ§Ã£o responsiva
-   Ia generativa - Modelos open-source 

## ğŸ“¦ InstalaÃ§Ã£o  

Clone o repositÃ³rio e instale as dependÃªncias:  

cd hackathona  
npm install  

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente  

Crie um arquivo `.env` na raiz do projeto e adicione as chaves do Firebase:  

VITE_API_KEY="AIzaS...."  
VITE_AUTH_DOMAIN="hack..."  
VITE_PROJECT_ID="hackatonamob4"  
VITE_STORAGE_BUCKET="hack..."  
VITE_MESSAGING_SENDER_ID="50..."  
VITE_APP_ID="1:507969476135:web:1a2..."  
VITE_MEASUREMENT_ID="G-X..."  

Importante: nunca exponha suas prÃ³prias chaves de produÃ§Ã£o em repositÃ³rios pÃºblicos.  
As chaves acima sÃ£o de exemplo para rodar o projeto localmente.  

## â–¶ï¸ Rodando o projeto  

npm run dev  

O app ficarÃ¡ disponÃ­vel em:  
http://localhost:5173  

## ğŸ¤ Contribuindo  

Este Ã© um projeto open source. Toda contribuiÃ§Ã£o Ã© bem-vinda!  

1. FaÃ§a um fork  
2. Crie uma branch (git checkout -b minha-feature)  
3. Commit suas alteraÃ§Ãµes (git commit -m "feat: minha nova feature")  
4. FaÃ§a um push (git push origin minha-feature)  
5. Abra um Pull Request ğŸš€  

## ğŸ‘¨â€ğŸ’» Equipe  

IMPORT PANDAS ğŸ¼  
- Claudio Jr.  
- Nicolas Macedo  
- Bernardo Lopes  
- Juan Perri



## Chat API - Backend Flask

Backend Flask para o chatbot usando transformers e pipeline.

## Estrutura do Projeto

```
chat/
â”œâ”€â”€ app.py                 # AplicaÃ§Ã£o principal Flask
â”œâ”€â”€ run.py                 # Script para executar o servidor
â”œâ”€â”€ wsgi.py               # WSGI para produÃ§Ã£o
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ env.example          # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ README.md            # Esta documentaÃ§Ã£o
â”œâ”€â”€ config/              # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ models/              # Modelos de ML
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_model.py
â”œâ”€â”€ routes/              # Rotas da API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_routes.py
â”œâ”€â”€ services/            # ServiÃ§os
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_service.py
â””â”€â”€ utils/               # UtilitÃ¡rios
    â”œâ”€â”€ __init__.py
    â””â”€â”€ validators.py
```

## InstalaÃ§Ã£o

1. **Instale as dependÃªncias:**
```bash
cd chat
pip install -r requirements.txt
```

2. **Configure as variÃ¡veis de ambiente:**
```bash
cp env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

3. **Execute o servidor:**
```bash
python run.py
```

## Endpoints da API

### Health Check
- **GET** `/api/chat/health`
- Verifica se a API estÃ¡ funcionando

### InformaÃ§Ãµes do Modelo
- **GET** `/api/chat/model/info`
- Retorna informaÃ§Ãµes sobre o modelo carregado

### SessÃµes de Chat

#### Criar SessÃ£o
- **POST** `/api/chat/session/create`
- Cria uma nova sessÃ£o de chat
- **Resposta:**
```json
{
  "session_id": "uuid-da-sessao",
  "message": "SessÃ£o criada com sucesso"
}
```

#### Enviar Mensagem
- **POST** `/api/chat/session/{session_id}/message`
- Envia uma mensagem e recebe resposta
- **Body:**
```json
{
  "message": "Sua mensagem aqui",
  "max_length": 1000,
  "temperature": 0.7
}
```
- **Resposta:**
```json
{
  "response": "Resposta do modelo",
  "session_id": "uuid-da-sessao",
  "timestamp": "2024-01-01T12:00:00",
  "model": "microsoft/DialoGPT-medium"
}
```

#### Obter HistÃ³rico
- **GET** `/api/chat/session/{session_id}/history`
- Retorna o histÃ³rico de mensagens da sessÃ£o

#### Limpar SessÃ£o
- **DELETE** `/api/chat/session/{session_id}/clear`
- Limpa o histÃ³rico da sessÃ£o

#### Streaming (SSE)
- **POST** `/api/chat/session/{session_id}/stream`
- Envia mensagem e recebe resposta em streaming
- Usa Server-Sent Events (SSE)

## ConfiguraÃ§Ãµes

### VariÃ¡veis de Ambiente

- `SECRET_KEY`: Chave secreta do Flask
- `DEBUG`: Modo debug (True/False)
- `PORT`: Porta do servidor (padrÃ£o: 5000)
- `DEFAULT_MODEL`: Modelo a ser usado (padrÃ£o: microsoft/DialoGPT-medium)
- `MAX_MESSAGE_LENGTH`: Comprimento mÃ¡ximo da mensagem
- `DEFAULT_TEMPERATURE`: Temperatura para geraÃ§Ã£o (0.0-2.0)
- `CORS_ORIGINS`: Origens permitidas para CORS

### Modelos DisponÃ­veis

- `microsoft/DialoGPT-small`: Modelo pequeno (117M parÃ¢metros)


## Desenvolvimento

### Executar em Desenvolvimento
```bash
python run.py
```

### Setup Automatizado (Recomendado)

Para configurar todo o ambiente (virtualenv, dependÃªncias, criaÃ§Ã£o de `.env`, download antecipado do modelo e opÃ§Ã£o 8-bit), use o script na raiz do projeto:

```bash
bash scripts/setup_local_llm.sh
```

OpÃ§Ãµes disponÃ­veis:

- `--model <NOME>`: forÃ§a um modelo (ex: `Qwen/Qwen1.8B-Chat`)
- `--simple`: usa `requirements_simple.txt` (sem transformers) â€“ Ãºtil para apenas API sem geraÃ§Ã£o
- `--8bit`: tenta instalar `bitsandbytes` e ativa carregamento 8-bit (GPU necessÃ¡ria)
- `--python <bin>`: especifica binÃ¡rio Python (ex: `python3.11`)

Exemplos:
```bash
# Setup padrÃ£o
bash scripts/setup_local_llm.sh

# Modelo especÃ­fico maior
bash scripts/setup_local_llm.sh --model Qwen/Qwen1.8B-Chat

# Ambiente leve + 8-bit
bash scripts/setup_local_llm.sh --simple --8bit --model Qwen/Qwen1.5-0.5B-Chat

# Usando Python 3.11
bash scripts/setup_local_llm.sh --python python3.11
```

Depois de rodar:
```bash
source .venv/bin/activate
python chat/run.py
```

### Executar em ProduÃ§Ã£o
```bash
gunicorn chat.wsgi:app
```

### Testar Endpoints
```bash

curl http://localhost:5000/api/chat/health


curl -X POST http://localhost:5000/api/chat/session/create

curl -X POST http://localhost:5000/api/chat/session/{session_id}/message \
  -H "Content-Type: application/json" \
  -d '{"message": "OlÃ¡, como vocÃª estÃ¡?"}'
```


As rotas da API seguem o padrÃ£o RESTful e retornam JSON.


## ğŸ“œ LicenÃ§a  

Este projeto Ã© licenciado sob a MIT License.  
Sinta-se livre para usar, modificar e compartilhar.  
